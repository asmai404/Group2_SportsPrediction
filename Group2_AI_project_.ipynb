{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CDy2Z49QZOlj"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import GridSearchCV, KFold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tIOlWf_xc1o9"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hy6w49vqZUDF"
      },
      "outputs": [],
      "source": [
        "# Read the CSV file with a maximum of 1,227,516 rows\n",
        "player_21 = pd.read_csv( '/content/drive/MyDrive/Colab Notebooks/Mid-Sem Project/players_21.csv')\n",
        "\n",
        "player_22 = pd.read_csv( '/content/drive/MyDrive/Colab Notebooks/Mid-Sem Project/players_22.csv')\n",
        "\n",
        "# to display all columns of the data set\n",
        "pd.set_option('display.max_columns', None)\n",
        "player_21 = player_21.drop(columns=['sofifa_id'])\n",
        "player_22 = player_22.drop(columns=['sofifa_id'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VhXWPrr9fvje"
      },
      "outputs": [],
      "source": [
        "player_21.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6tLos1c6fva4"
      },
      "outputs": [],
      "source": [
        "player_22.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QqRfqn21g-mx"
      },
      "outputs": [],
      "source": [
        "player_21.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WuLRXDWNg-jy"
      },
      "outputs": [],
      "source": [
        "player_22.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aVXGcsYrhSSZ"
      },
      "outputs": [],
      "source": [
        "## Dropping irrelevant columns: Categorical features\n",
        "cat_21 = player_21.select_dtypes(include=['object']).columns\n",
        "cat_22 = player_22.select_dtypes(include=['object']).columns\n",
        "\n",
        "#Skipping URLS\n",
        "t = ['url' not in c for c in cat_21]\n",
        "t2 = ['url' not in c for c in cat_22]\n",
        "\n",
        "cat_21 = cat_21[t]\n",
        "cat_21 = cat_22[t2]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuAExXplq5PS"
      },
      "source": [
        "FEATURE EXTRACTIONS FOR TEXT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vbNXcCiZhSOy"
      },
      "outputs": [],
      "source": [
        "player_21[cat_21]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9UnpvuSvhSCN"
      },
      "outputs": [],
      "source": [
        "### Impute missing values in\n",
        "\n",
        "## players_21:\n",
        "# Extract numerical features\n",
        "num_21 = player_21.select_dtypes(include=['int64', 'float64']).columns\n",
        "# Impute missing values in players_21\n",
        "num_imputer_21 = SimpleImputer(strategy='mean')\n",
        "\n",
        "player_21[num_21] = num_imputer_21.fit_transform(player_21[num_21])\n",
        "\n",
        "## players_22:\n",
        "# Extract numerical features\n",
        "num_22 = player_22.select_dtypes(include=['int64', 'float64']).columns\n",
        "# Impute missing values in players_22\n",
        "num_imputer_22 = SimpleImputer(strategy='mean')\n",
        "player_22[num_22] = num_imputer_22.fit_transform(player_22[num_22])\n",
        "\n",
        "\n",
        "#Filling in NaNs for text values (imputing with forward fill)\n",
        "player_21[cat_21].fillna(method='ffill', inplace=True)\n",
        "player_22[cat_22].fillna(method='ffill', inplace=True)\n",
        "\n",
        "\n",
        "#coverting texts into numerical values with encoder\n",
        "enc = LabelEncoder()\n",
        "for c in player_21[cat_21]:\n",
        "  player_21[c] = enc.fit_transform(player_21[c])\n",
        "\n",
        "\n",
        "enc2 = LabelEncoder()\n",
        "for c2 in player_22[cat_22]:\n",
        "  player_22[c2] = enc.fit_transform(player_22[c2])\n",
        "\n",
        "# Merging textual and numerical data for training\n",
        "merge_num_text_21 = pd.concat([player_21[num_21], player_21[cat_21]], axis=0)\n",
        "merge_num_text_22 = pd.concat([player_22[num_22], player_22[cat_22]], axis=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# nanValues = merge_num_text_21.isna()\n",
        "# merge_num_text_21= merge_num_text_21[nanValues]"
      ],
      "metadata": {
        "id": "Wg8aa3Lb3RLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIRU6X4dzFLZ"
      },
      "source": [
        "Using correlation to find the dependent variables that are strongly contributing to the players overall\n",
        "INDEPENDENT VARIABLE: Overall\n",
        "DEPENDENT: Remaining variables\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F2VZzxxLGOci"
      },
      "outputs": [],
      "source": [
        "corr_matrix = merge_num_text_21.corr()\n",
        "corr_matrix_22 = player_22.corr()\n",
        "corr_matrix\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhMlRIYeyhU3"
      },
      "source": [
        "Identifying the key variables that strongly contribute to the players overall according to the correlation analysis on the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3D-3bHTykMQj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W9WomOqiGjVc"
      },
      "outputs": [],
      "source": [
        "corr_matrix_22['overall'].sort_values(ascending=False)\n",
        "corr_matrix['overall'].sort_values(ascending=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "js8Ok_cUzGEv"
      },
      "source": [
        "Identifying the key features that characterize the dependent variables. Dropping all other variables except the below variables obtained from the correlation analysis."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Threshold for correlation relevance is set to 50%\n",
        "corr_matrix['overall'] > 0.5"
      ],
      "metadata": {
        "id": "3vjP8ukl9XZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "twtEdWNS8Yzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "54RAzC9iGpsY"
      },
      "outputs": [],
      "source": [
        "# List of feature names with highest correlation to 'overall'\n",
        "selected_features = ['movement_reactions','passing','mentality_composure','dribbling','potential','release_clause_eur','wage_eur',\n",
        "    'value_eur','power_shot_power','physic','mentality_vision','attacking_short_passing'\n",
        "]\n",
        "# Select these features from the dataset\n",
        "selected_data = player_21[['overall'] + selected_features]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tv2Oz4uy0Oj7"
      },
      "source": [
        "Presenting the Dependent Variables Alongside Their Various Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lu1eGQh42BZZ"
      },
      "source": [
        "Ensuring that all missing values have been effectively substituted with their respective means."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PS4_ZSdK_iN4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oww4RCbsyceM"
      },
      "source": [
        "Dividing the dataset into two distinct components: the Y-variable, representing the target or dependent variable, and the X-variables, which encompass the independent variables or features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWr4ihPNClag"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# Specify the features (X) and the target variable (y)\n",
        "y = selected_data['overall']  # Target variable\n",
        "X = selected_data.drop(columns=['overall'])  # Features\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLnv0wrBgL7O"
      },
      "source": [
        "Before standardizing the dependent variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cK--MyeqsOV1"
      },
      "outputs": [],
      "source": [
        "X"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LP70WGPk4DOa"
      },
      "source": [
        "Standardizing the dependent variables to ensure uniform measurement units across all of them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vWVVUYXhgTda"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "scaled = sc.fit_transform(X)\n",
        "X = pd.DataFrame(scaled, columns=X.columns)\n",
        "X"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7L51_ZDrppnL"
      },
      "source": [
        "Create the models, initialize and train the models to get ready for prediction\n",
        "Using Ensemble Learning"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and testing sets (e.g., 80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "62lFMe_L-IBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'max_depth': [3, 4, 5],\n",
        "}\n",
        "\n",
        "regressorGB = GradientBoostingRegressor()\n",
        "grid_searchGB = GridSearchCV(estimator=regressorGB, param_grid=param_grid, cv=KFold(n_splits=5), scoring='neg_mean_squared_error')\n",
        "grid_searchGB.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "U9bqkLmP_U_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grid_searchGB.best_score_"
      ],
      "metadata": {
        "id": "bRmb4fiiQrPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [None, 10, 20],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "}\n",
        "\n",
        "regressorRF = RandomForestRegressor()\n",
        "grid_searchRF = GridSearchCV(estimator=regressorRF, param_grid=param_grid, cv=KFold(n_splits=5), scoring='neg_mean_squared_error')\n",
        "grid_searchRF.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "Qpf4DV0UAIqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iJAnjKi_RcGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'max_depth': [3, 4, 5],\n",
        "}\n",
        "\n",
        "regressorXGB = xgb.XGBRegressor()\n",
        "grid_searchXGB = GridSearchCV(estimator=regressorXGB, param_grid=param_grid, cv=KFold(n_splits=5), scoring='neg_mean_squared_error')\n",
        "grid_searchXGB.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "w6VQgdmzBMM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = grid_searchGB\n",
        "best_params = grid_searchGB.get_params\n",
        "\n",
        "if grid_searchGB.best_score_ > grid_searchRF.best_score_ and grid_searchGB.best_score_ > grid_searchXGB.best_score_:\n",
        "    best_model = grid_searchGB\n",
        "    best_params = grid_searchGB.best_params_\n",
        "elif grid_searchRF.best_score_ > grid_searchGB.best_score_ and grid_searchRF.best_score_ > grid_searchXGB.best_score_:\n",
        "    best_model = grid_searchRF\n",
        "    best_params = grid_searchRF.best_params_\n",
        "else:\n",
        "    best_model = grid_searchXGB\n",
        "    best_params = grid_searchXGB.best_params_\n"
      ],
      "metadata": {
        "id": "t5lJCL-TFjn8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SAVING THE MODEL AS A PICKLE FILE"
      ],
      "metadata": {
        "id": "uwVmxCgOH-iE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Save the model to a file\n",
        "with open('football_prediction_model.pkl', 'wb') as model_file:\n",
        "    pickle.dump(best_model, model_file)\n"
      ],
      "metadata": {
        "id": "lsHnZQCjH97h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "igXTXHasCswY"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DbiE8LSv68S"
      },
      "source": [
        "Creating a linear regression model and training it with the dataset \"Players_21"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UKoqZ4J7mlDY"
      },
      "outputs": [],
      "source": [
        "# from sklearn.linear_model import LinearRegression\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.metrics import mean_squared_error, r2_score\n",
        "# # Create a Linear Regression model\n",
        "# model = LinearRegression()\n",
        "# # Train the model on the training dataset\n",
        "# model.fit(X_train, y_train)\n",
        "# # Split the dataset into training and testing sets\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "# # Make predictions on the testing set\n",
        "# y_pred = model.predict(X_test)\n",
        "# # Calculate the Mean Squared Error (MSE)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xBUs6Zvv-Qx8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeqxqEflwRun"
      },
      "source": [
        "Creating a RandomForestRegressor model and training it with the dataset \"Players_21"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H1CVw8_ent7l"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error\n",
        "# Create a RandomForest Regressor model\n",
        "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "# Train the model on the entire dataset\n",
        "model.fit(X, y)\n",
        "# Perform cross-validation to evaluate the model\n",
        "# Assuming X is your feature matrix and y is the target variable\n",
        "cross_val_scores = cross_val_score(model, X, y, cv=5, scoring='neg_mean_squared_error')\n",
        "# Calculate the mean of the cross-validation scores\n",
        "mean_cross_val_score = np.mean(-cross_val_scores)\n",
        "# Make predictions on the testing set\n",
        "y_pred = model.predict(X_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYPIXQW6wYak"
      },
      "source": [
        "Creating a xgb model and training it with the dataset \"Players_21"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ajeM5BaQpe6R"
      },
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import numpy as np\n",
        "# Create an XGBoost Regressor model\n",
        "model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
        "# Perform cross-validation to evaluate the model\n",
        "# Assuming X is your feature matrix and y is the target variable\n",
        "cross_val_scores = cross_val_score(model, X, y, cv=5, scoring='neg_mean_squared_error')\n",
        "# Calculate the mean of the cross-validation scores (MSE)\n",
        "mean_cross_val_mse = np.mean(-cross_val_scores)\n",
        "# Train the model on the entire dataset\n",
        "model.fit(X, y)\n",
        "# Make predictions on the training data\n",
        "y_pred = model.predict(X)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DG9eoHPxwiyy"
      },
      "source": [
        "Creating ensemble for the 3 model which are xgb,RandomForestRegressor and linear Regression models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bSroUJRDw4_6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G6atMOckyNtw"
      },
      "outputs": [],
      "source": [
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train individual models\n",
        "xgb_model = xgb.XGBRegressor()\n",
        "rf_model = RandomForestRegressor()\n",
        "lr_model = LinearRegression() #gradiebt\n",
        "\n",
        "xgb_model.fit(X_train, y_train)\n",
        "rf_model.fit(X_train, y_train)\n",
        "lr_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions with each model\n",
        "xgb_pred = xgb_model.predict(X_test)\n",
        "rf_pred = rf_model.predict(X_test)\n",
        "lr_pred = lr_model.predict(X_test)\n",
        "\n",
        "# Create an ensemble by averaging the predictions\n",
        "ensemble_pred = (xgb_pred + rf_pred + lr_pred) / 3\n",
        "\n",
        "# Calculate the ensemble's RMSE (Root Mean Squared Error)\n",
        "ensemble_rmse = np.sqrt(mean_squared_error(y_test, ensemble_pred))\n",
        "\n",
        "print(f\"XGBoost RMSE: {np.sqrt(mean_squared_error(y_test, xgb_pred))}\")\n",
        "print(f\"Random Forest RMSE: {np.sqrt(mean_squared_error(y_test, rf_pred))}\")\n",
        "print(f\"Linear Regression RMSE: {np.sqrt(mean_squared_error(y_test, lr_pred))}\")\n",
        "print(f\"Ensemble RMSE: {ensemble_rmse}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}